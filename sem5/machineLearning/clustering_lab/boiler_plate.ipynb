{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from scipy.cluster.hierarchy import linkage, dendrogram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Edit this section\n",
    "\n",
    "* Load_data:\n",
    "    * load data from csv using `\\t` as the separator.\n",
    "    * Drop irrelevant columns\n",
    "    * Label encode\n",
    "    * select the main features and make an array of values (an array of arrays. say `x`)\n",
    "    * Scalarization of the features\n",
    "    * return `scaled_x` variable which is a variable containing the scaled values of x\n",
    "\n",
    "* apply_pca:\n",
    "    * return `x_pca` variable which is the set of reduced features.\n",
    "    * for ideal results, use n_components as 1. Feel free to experiment with this value but n_components = 1 is ideal case.\n",
    "\n",
    "* find_optimal_clusters:\n",
    "    * this is to find the optimal number of clusters using the elbow method.\n",
    "    * find the values of inertia till max_clusters+1 and append the inertia values to an array.\n",
    "    * return the inertia array with `inertia` variable. `VERY IMPORTANT TO SEE THE GRAPH`\n",
    "\n",
    "* perform_kmeans_clustering:\n",
    "    * use kmeans `init = \"k-means++\"`, `max_iter=300`, `n_init = 10`, `random_state` is your choice and `n_clusters` is the best elbow value.\n",
    "    * return the kmeans fit predict values with `y_kmeans` variable.\n",
    "\n",
    "* perform_agglomerative_clustering:\n",
    "    * Linkage method is \"ward\" and n_clusters is the best elbow value.\n",
    "    * return the agglomerative clustering values after fit predicting the `x` feature values as `y_agglomerative` variable name.\n",
    "\n",
    "* for dendrogram:\n",
    "    * get_linkages:\n",
    "        * it helps to form the linkages between x and in the 'ward' method.\n",
    "        * return `linked` variable which has the linkages.\n",
    "    * plot_dendrogram:\n",
    "        * it is to plot the dendrogram in the (10,7) with x_label as index and y_label as distance\n",
    "        * dendrogram function takes in the linkages from `get_linkages` function as the `linked` variable.\n",
    "\n",
    "* Now, you are requested to submit screenshots of the `dengrogram, test case 1 and 2 passed messages`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(filepath):\n",
    "    # Load the data from the file, its tab separated\n",
    "    # load data from csv using `\\t` as the separator.\n",
    "    # Drop irrelevant columns\n",
    "    # Label encode\n",
    "    # select the main features and make an array of values (an array of arrays. say `x`)\n",
    "    # Scalarization of the features\n",
    "    # return `scaled_x` variable which is a variable containing the scaled values of x\n",
    "    data = pd.read_csv('dataset1.csv', sep='\\t')\n",
    "    data = data.drop(['id', 'diagnosis', 'Unnamed: 32'], axis=1)\n",
    "    data['diagnosis'] = data['diagnosis'].map({'M': 1, 'B': 0})\n",
    "    x = data.iloc[:, 1:].values\n",
    "    scaler = StandardScaler()\n",
    "    scaled_x = scaler.fit_transform(x)\n",
    "    return scaled_x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_pca(x, n_components=1):\n",
    "    pca = PCA(n_components=n_components)\n",
    "    pca.fit(x)\n",
    "    return pca\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_optimal_clusters(x, max_clusters=10):\n",
    "    # this is to find the optimal number of clusters using the elbow method.\n",
    "    # find the values of inertia till max_clusters+1 and append the inertia values to an array.\n",
    "    # return the inertia array with `inertia` variable. `VERY IMPORTANT TO SEE THE GRAPH`\n",
    "    inertia = []\n",
    "    for n in range(1, max_clusters + 1):\n",
    "        kmeans = KMeans(n_clusters=n, init='k-means++', max_iter=300, n_init=10, random_state=42)\n",
    "        kmeans.fit(x)\n",
    "        inertia.append(kmeans.inertia_)\n",
    "    \n",
    "    plt.figure(figsize=(10, 7))\n",
    "    plt.plot(range(1, max_clusters + 1), inertia, marker='o')\n",
    "    plt.xlabel('Number of clusters')\n",
    "    plt.ylabel('Inertia')\n",
    "    plt.title('Elbow Method For Optimal Number of Clusters')\n",
    "    plt.show()\n",
    "    \n",
    "    return inertia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform KMeans Clustering\n",
    "def perform_kmeans_clustering(x, n_clusters=None):\n",
    "    pass\n",
    "\n",
    "# Perform Agglomerative Clustering\n",
    "def perform_agglomerative_clustering(x, n_clusters=None):\n",
    "    pass\n",
    "\n",
    "# Plot the dendrogram for hierarchical clustering\n",
    "def plot_dendrogram(linked):\n",
    "    pass\n",
    "\n",
    "# Linkage for Hierarchical Clustering\n",
    "def get_linkages(x):\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Just edit the file path here... Don't edit anything else"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Silhouette Score Calculation\n",
    "def calculate_silhouette_score(x, y_pred):\n",
    "    return silhouette_score(x, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dataset1' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 37\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[1;32m     28\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minertia\u001b[39m\u001b[38;5;124m\"\u001b[39m: inertia,\n\u001b[1;32m     29\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msilhouette_kmeans\u001b[39m\u001b[38;5;124m\"\u001b[39m: silhouette_kmeans,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpca_components\u001b[39m\u001b[38;5;124m\"\u001b[39m: x_pca\n\u001b[1;32m     34\u001b[0m     }\n\u001b[1;32m     36\u001b[0m \u001b[38;5;66;03m# Call the main function and print silhouette scores and dendrogram\u001b[39;00m\n\u001b[0;32m---> 37\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdataset1.csv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mKMeans Silhouette Score: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msilhouette_kmeans\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAgglomerative Silhouette Score: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msilhouette_agglomerative\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[19], line 2\u001b[0m, in \u001b[0;36mmain\u001b[0;34m(filepath)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmain\u001b[39m(filepath):\n\u001b[0;32m----> 2\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[43mload_data\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdataset1.csv\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;66;03m# Apply PCA for Dimensionality Reduction\u001b[39;00m\n\u001b[1;32m      5\u001b[0m     x_pca \u001b[38;5;241m=\u001b[39m apply_pca(x, n_components\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "Cell \u001b[0;32mIn[3], line 9\u001b[0m, in \u001b[0;36mload_data\u001b[0;34m(filepath)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_data\u001b[39m(filepath):\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;66;03m# Load the data from the file, its tab separated\u001b[39;00m\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;66;03m# load data from csv using `\\t` as the separator.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;66;03m# Scalarization of the features\u001b[39;00m\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;66;03m# return `scaled_x` variable which is a variable containing the scaled values of x\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m     data \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[43mdataset1\u001b[49m\u001b[38;5;241m.\u001b[39mcsv, sep\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     10\u001b[0m     data \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mdrop([\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mid\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdiagnosis\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mUnnamed: 32\u001b[39m\u001b[38;5;124m'\u001b[39m], axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     11\u001b[0m     data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdiagnosis\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdiagnosis\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mmap({\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mM\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m1\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mB\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m0\u001b[39m})\n",
      "\u001b[0;31mNameError\u001b[0m: name 'dataset1' is not defined"
     ]
    }
   ],
   "source": [
    "def main(filepath):\n",
    "    x = load_data('dataset1.csv')\n",
    "\n",
    "    # Apply PCA for Dimensionality Reduction\n",
    "    x_pca = apply_pca(x, n_components=1)\n",
    "    \n",
    "    # KMeans: finding optimal clusters\n",
    "    inertia = find_optimal_clusters(x_pca)\n",
    "    \n",
    "    # KMeans clustering implementation\n",
    "    y_kmeans = perform_kmeans_clustering(x_pca)\n",
    "    \n",
    "    # Agglomerative Clustering\n",
    "    y_agglomerative = perform_agglomerative_clustering(x_pca)\n",
    "    \n",
    "    # Hierarchical Clustering Dendrogram\n",
    "    hierarchical_linked = get_linkages(x_pca)\n",
    "    \n",
    "    # Plot the dendrogram for hierarchical clustering\n",
    "    plot_dendrogram(hierarchical_linked)\n",
    "    \n",
    "    # Silhouette scores\n",
    "    silhouette_kmeans = calculate_silhouette_score(x_pca, y_kmeans)\n",
    "    silhouette_agglomerative = calculate_silhouette_score(x_pca, y_agglomerative)\n",
    "    \n",
    "    # Return results\n",
    "    return {\n",
    "        \"inertia\": inertia,\n",
    "        \"silhouette_kmeans\": silhouette_kmeans,\n",
    "        \"silhouette_agglomerative\": silhouette_agglomerative,\n",
    "        \"agglomerative_clustering\": y_agglomerative,\n",
    "        \"hierarchical_linked\": hierarchical_linked,\n",
    "        \"pca_components\": x_pca\n",
    "    }\n",
    "\n",
    "# Call the main function and print silhouette scores and dendrogram\n",
    "result = main(\"dataset1.csv\")\n",
    "print(f\"KMeans Silhouette Score: {result['silhouette_kmeans']}\")\n",
    "print(f\"Agglomerative Silhouette Score: {result['silhouette_agglomerative']}\")\n",
    "\n",
    "# Silhouette scores checks \n",
    "if result['silhouette_kmeans'] >= 0.5:\n",
    "    print(\"Test Case 1 passed!\")\n",
    "if result['silhouette_agglomerative'] >= 0.5:\n",
    "    print(\"Test case 2 passed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "\n",
    "# # Load the CSV file\n",
    "# data_main = pd.read_csv(\"C:/Users/aniru/Desktop/Clustering/Random_DS_Implementation/dataset.csv\")\n",
    "\n",
    "# # Drop the 'id' column and features from 'f_07' to 'f_28'\n",
    "# data_main.drop(columns=['id'] + [f'f_{i:02d}' for i in range(8, 29)], inplace=True)\n",
    "\n",
    "# # Save the modified dataset to a new CSV file\n",
    "# data_main.to_csv(\"your_modified_dataset.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
